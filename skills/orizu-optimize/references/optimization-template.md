# Optimization Script Template

Use this template to generate the `optimize.py` script for users.

## Full Template

```python
"""
Observable GEPA Optimization Script
Generated by Orizu Prompt Optimization

Project: [PROJECT_NAME]
Task: [TASK_DESCRIPTION]
"""

import dspy
from dotenv import load_dotenv
import os
import sys
import random
import pandas as pd

from gepa_observable import GEPA

# =============================================================================
# LOAD ENVIRONMENT
# =============================================================================
load_dotenv(".env.local")

# =============================================================================
# MODEL CONFIGURATION
# Replace these with user-specified values
# =============================================================================
PROGRAM_LM = "[PROGRAM_LM]"        # e.g., "openai/gpt-4o-mini"
REFLECTIVE_LM = "[REFLECTIVE_LM]"  # e.g., "openai/gpt-4o"
JUDGE_LM = "[JUDGE_LM]"            # e.g., "openai/gpt-4o" (if using LLM-as-judge)

PROJECT_NAME = "[PROJECT_NAME]"
BUDGET = "[BUDGET]"  # "light", "medium", "heavy", or custom

# =============================================================================
# VALIDATE API KEYS
# =============================================================================
def validate_api_keys():
    """Validate required API keys are set in .env.local"""
    required_keys = set()

    for model in [PROGRAM_LM, REFLECTIVE_LM, JUDGE_LM]:
        if not model or model.startswith("["):
            continue
        if model.startswith("openai/"):
            required_keys.add("OPENAI_API_KEY")
        elif model.startswith("anthropic/"):
            required_keys.add("ANTHROPIC_API_KEY")
        elif model.startswith("google/"):
            required_keys.add("GOOGLE_API_KEY")

    missing = [key for key in required_keys if not os.getenv(key)]
    if missing:
        print("=" * 60)
        print("ERROR: Missing API keys in .env.local")
        print("=" * 60)
        print(f"\nPlease add the following to your .env.local file:\n")
        for key in missing:
            print(f"  {key}=your-key-here")
        print()
        sys.exit(1)

    print("API keys validated successfully.")

validate_api_keys()

# =============================================================================
# CONFIGURE LANGUAGE MODELS
# =============================================================================
program_lm = dspy.LM(PROGRAM_LM, temperature=1.0)
reflective_lm = dspy.LM(REFLECTIVE_LM, temperature=1.0)
dspy.configure(lm=program_lm)

# If using LLM-as-judge, configure it here
# judge_lm = dspy.LM(JUDGE_LM, temperature=0.0)

print(f"Program LM: {PROGRAM_LM}")
print(f"Reflective LM: {REFLECTIVE_LM}")

# =============================================================================
# DEFINE THE DSPY PROGRAM
# Replace with user's task-specific signature and program
# =============================================================================
class TaskSignature(dspy.Signature):
    """[TASK_DOCSTRING]"""

    [INPUT_FIELD]: str = dspy.InputField(desc="[INPUT_DESCRIPTION]")
    [OUTPUT_FIELD]: str = dspy.OutputField(desc="[OUTPUT_DESCRIPTION]")

# Choose program type based on task complexity
program = dspy.ChainOfThought(TaskSignature)  # or dspy.Predict(TaskSignature)

# =============================================================================
# LOAD AND SPLIT DATA
# =============================================================================
def load_data(file_path: str) -> list[dict]:
    """Load data from CSV, JSON, or JSONL."""
    if file_path.endswith(".csv"):
        df = pd.read_csv(file_path)
        return df.to_dict(orient="records")
    elif file_path.endswith(".json"):
        import json
        with open(file_path) as f:
            return json.load(f)
    elif file_path.endswith(".jsonl"):
        import json
        data = []
        with open(file_path) as f:
            for line in f:
                data.append(json.loads(line.strip()))
        return data
    else:
        raise ValueError(f"Unsupported file format: {file_path}")

def create_examples(data: list[dict], input_field: str, output_field: str) -> list:
    """Convert raw data to DSPy Examples."""
    examples = []
    for row in data:
        example = dspy.Example(
            **{input_field: row[input_field], output_field: row[output_field]}
        ).with_inputs(input_field)
        examples.append(example)
    return examples

# Load data
raw_data = load_data("[DATA_FILE_PATH]")
examples = create_examples(raw_data, "[INPUT_FIELD]", "[OUTPUT_FIELD]")

# Shuffle and split
random.seed(42)
random.shuffle(examples)

train_size = int(len(examples) * 0.7)
val_size = int(len(examples) * 0.15)

train_data = examples[:train_size]
val_data = examples[train_size:train_size + val_size]
test_data = examples[train_size + val_size:]

print(f"\nData splits:")
print(f"  Train: {len(train_data)} examples")
print(f"  Val: {len(val_data)} examples")
print(f"  Test: {len(test_data)} examples")

# =============================================================================
# IMPORT SCORERS
# =============================================================================
from scorers import metric

# =============================================================================
# RUN OPTIMIZATION
# =============================================================================
print("\n" + "=" * 60)
print("Starting GEPA Optimization")
print("=" * 60)

# Configure budget
budget_kwargs = {}
if BUDGET in ["light", "medium", "heavy"]:
    budget_kwargs["auto"] = BUDGET
elif BUDGET.startswith("max_full_evals="):
    budget_kwargs["max_full_evals"] = int(BUDGET.split("=")[1])
elif BUDGET.startswith("max_metric_calls="):
    budget_kwargs["max_metric_calls"] = int(BUDGET.split("=")[1])
else:
    budget_kwargs["auto"] = "medium"  # Default

optimizer = GEPA(
    metric=metric,
    **budget_kwargs,
    reflection_lm=reflective_lm,

    # Observable features
    server_url="http://localhost:3000",
    project_name=PROJECT_NAME,
    verbose=True,
    capture_lm_calls=True,
    capture_stdout=True,
)

print(f"\nView your optimization at: http://localhost:3000")
print(f"Project: {PROJECT_NAME}")
print()

# Run optimization
optimized = optimizer.compile(
    student=program,
    trainset=train_data,
    valset=val_data,
)

# =============================================================================
# RESULTS
# =============================================================================
print("\n" + "=" * 60)
print("OPTIMIZATION COMPLETE")
print("=" * 60)

# Save optimized program
optimized.save("optimized_program.json")
print(f"\nOptimized program saved to: optimized_program.json")

# Show final prompt
print("\n--- Final Optimized Prompt ---")
for name, pred in optimized.named_predictors():
    print(f"\n{name}:")
    print(pred.signature.instructions)

print("\n" + "=" * 60)
print(f"View full results at: http://localhost:3000")
print("=" * 60)
```

## Placeholders to Replace

When generating the script, replace these placeholders:

| Placeholder | Description | Example |
|-------------|-------------|---------|
| `[PROJECT_NAME]` | User's project name | `"Math QA Optimization"` |
| `[TASK_DESCRIPTION]` | Brief task description | `"Answer math questions"` |
| `[PROGRAM_LM]` | Model for the program | `"openai/gpt-4o-mini"` |
| `[REFLECTIVE_LM]` | Model for reflection | `"openai/gpt-4o"` |
| `[JUDGE_LM]` | Model for LLM-as-judge | `"openai/gpt-4o"` |
| `[BUDGET]` | Budget configuration | `"medium"` or `"max_full_evals=5"` |
| `[DATA_FILE_PATH]` | Path to data file | `"data/evals.csv"` |
| `[INPUT_FIELD]` | Input field name | `"question"` |
| `[OUTPUT_FIELD]` | Output field name | `"answer"` |
| `[TASK_DOCSTRING]` | Signature docstring | `"Answer the question accurately"` |
| `[INPUT_DESCRIPTION]` | Input field desc | `"The question to answer"` |
| `[OUTPUT_DESCRIPTION]` | Output field desc | `"The correct answer"` |

## Minimal Example (Filled In)

```python
"""Observable GEPA Optimization - Math QA"""

import dspy
from dotenv import load_dotenv
import os
import sys
import random
import pandas as pd

from gepa_observable import GEPA

load_dotenv(".env.local")

# Models
PROGRAM_LM = "openai/gpt-4o-mini"
REFLECTIVE_LM = "openai/gpt-4o"
PROJECT_NAME = "Math QA Optimization"

# Validate keys
if not os.getenv("OPENAI_API_KEY"):
    print("ERROR: OPENAI_API_KEY not found in .env.local")
    sys.exit(1)

# Configure LMs
program_lm = dspy.LM(PROGRAM_LM, temperature=1.0)
reflective_lm = dspy.LM(REFLECTIVE_LM, temperature=1.0)
dspy.configure(lm=program_lm)

# Program
class MathQA(dspy.Signature):
    """Answer the math question accurately."""
    question: str = dspy.InputField(desc="The math question")
    answer: str = dspy.OutputField(desc="The correct answer")

program = dspy.ChainOfThought(MathQA)

# Load data
df = pd.read_csv("data/evals.csv")
examples = [
    dspy.Example(question=row["question"], answer=row["answer"]).with_inputs("question")
    for _, row in df.iterrows()
]

random.seed(42)
random.shuffle(examples)
train_data = examples[:15]
val_data = examples[15:20]

# Import scorers
from scorers import metric

# Optimize
optimizer = GEPA(
    metric=metric,
    auto="medium",
    reflection_lm=reflective_lm,
    server_url="http://localhost:3000",
    project_name=PROJECT_NAME,
    verbose=True,
)

print(f"View at: http://localhost:3000")

optimized = optimizer.compile(
    student=program,
    trainset=train_data,
    valset=val_data,
)

optimized.save("optimized_program.json")
print("Done! Saved to optimized_program.json")
```
