# Single File Template

Complete working example in one `.py` file. Use this when you want everything self-contained and easy to run.

---

## Template

Replace placeholders (marked with `{PLACEHOLDER}`) based on user requirements.

```python
#!/usr/bin/env python3
"""
{PROJECT_NAME} - DSPy GEPA Optimization with Observability
Generated by observable-gepa-optimizer skill

Task: {TASK_DESCRIPTION}
Data: {DATA_SOURCE}
"""

import base64
import dspy
import pandas as pd
from pathlib import Path
from gepa_observable import GEPA

# =============================================================================
# Configuration
# =============================================================================

# LLM Configuration
LM_MODEL = "{LM_MODEL}"  # e.g., "anthropic/claude-3-5-sonnet-latest"
REFLECTION_LM = "{REFLECTION_LM}"  # e.g., "openai/gpt-4o"

# Observability
SERVER_URL = "{SERVER_URL}"  # e.g., "http://localhost:3000" or None
PROJECT_NAME = "{PROJECT_NAME}"

# Data paths
DATA_PATH = "{DATA_PATH}"  # e.g., "data/training.csv"
{FILE_DIR_CONFIG}  # FILE_DIR = "data/files" if applicable

# =============================================================================
# DSPy Signature
# =============================================================================

class {SIGNATURE_NAME}(dspy.Signature):
    """{SIGNATURE_DOCSTRING}"""

{INPUT_FIELDS}

{OUTPUT_FIELDS}

# =============================================================================
# DSPy Module
# =============================================================================

program = dspy.{MODULE_TYPE}({SIGNATURE_NAME})

# =============================================================================
# Utility Functions
# =============================================================================

{UTILITY_FUNCTIONS}

# =============================================================================
# Data Loading
# =============================================================================

def load_data():
    """Load and split data into train/val sets."""
{DATA_LOADING_CODE}

# =============================================================================
# Metric Function
# =============================================================================

def metric(gold, pred, trace=None, pred_name=None, pred_trace=None):
    """Evaluate prediction against ground truth with feedback."""
{METRIC_CODE}

# =============================================================================
# Main
# =============================================================================

def main():
    # Configure LM
    lm = dspy.LM(LM_MODEL)
    reflection_lm = dspy.LM(REFLECTION_LM)
    dspy.configure(lm=lm)

    # Load data
    train_data, val_data = load_data()
    print(f"Loaded {len(train_data)} training, {len(val_data)} validation examples")

    # Create optimizer
    optimizer = GEPA(
        metric=metric,
        auto="medium",
        reflection_lm=reflection_lm,
        server_url=SERVER_URL,
        project_name=PROJECT_NAME,
        capture_lm_calls=True,
        verbose=True,
    )

    # Optimize
    print("Starting GEPA optimization...")
    optimized = optimizer.compile(
        student=program,
        trainset=train_data,
        valset=val_data,
    )

    # Save optimized program
    output_path = "{OUTPUT_NAME}_optimized.json"
    optimized.save(output_path)
    print(f"Saved optimized program to {output_path}")

    # Test on first validation example
    if val_data:
        print("\nTesting optimized program...")
        test_example = val_data[0]
        result = optimized({TEST_CALL_ARGS})
        print(f"Test result: {result}")

    return optimized


if __name__ == "__main__":
    main()
```

---

## Example: Filled Template

Here's an example of the template filled in for a document extraction task:

```python
#!/usr/bin/env python3
"""
Real Estate Extraction - DSPy GEPA Optimization with Observability
Generated by observable-gepa-optimizer skill

Task: Extract transaction details from real estate PDFs
Data: data.csv + downloaded_files/
"""

import base64
import dspy
import pandas as pd
from pathlib import Path
from difflib import SequenceMatcher
from gepa_observable import GEPA

# =============================================================================
# Configuration
# =============================================================================

LM_MODEL = "anthropic/claude-3-5-sonnet-latest"
REFLECTION_LM = "openai/gpt-4o"
SERVER_URL = "http://localhost:3000"
PROJECT_NAME = "Real Estate Extraction"

DATA_PATH = "data.csv"
FILE_DIR = "downloaded_files"

# =============================================================================
# DSPy Signature
# =============================================================================

class RealEstateExtraction(dspy.Signature):
    """Extract real estate transaction details from purchase agreement PDF."""

    pdf_content: str = dspy.InputField(desc="Base64-encoded PDF content")
    matter_id: str = dspy.InputField(desc="Matter ID for reference")

    key_date: str = dspy.OutputField(desc="Closing date in YYYY-MM-DD format")
    purchase_amount: float = dspy.OutputField(desc="Purchase amount in dollars")
    kind: str = dspy.OutputField(desc="Transaction type: PURCHASE or SALE")
    purchaser_names: str = dspy.OutputField(desc="Comma-separated purchaser names")
    seller_names: str = dspy.OutputField(desc="Comma-separated seller names")

# =============================================================================
# DSPy Module
# =============================================================================

program = dspy.ChainOfThought(RealEstateExtraction)

# =============================================================================
# Utility Functions
# =============================================================================

def load_pdf_for_llm(pdf_path: str) -> str:
    """Load PDF as base64 for multimodal LLM."""
    with open(pdf_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")


def fuzzy_name_match(gold_names: str, pred_names: str) -> float:
    """Compare names with fuzzy matching, order-independent."""
    gold_set = set(n.strip().lower() for n in str(gold_names).split(','))
    pred_set = set(n.strip().lower() for n in str(pred_names).split(','))

    if gold_set == pred_set:
        return 1.0

    matched = 0
    for g in gold_set:
        best = max((SequenceMatcher(None, g, p).ratio() for p in pred_set), default=0)
        if best > 0.85:
            matched += 1

    return matched / len(gold_set) if gold_set else 0.0

# =============================================================================
# Data Loading
# =============================================================================

def load_data():
    """Load data with PDF documents."""
    df = pd.read_csv(DATA_PATH)
    examples = []

    for _, row in df.iterrows():
        matter_id = row['Matter ID']
        pdf_path = Path(FILE_DIR) / f"{matter_id}.pdf"

        if not pdf_path.exists():
            continue

        pdf_content = load_pdf_for_llm(str(pdf_path))

        example = dspy.Example(
            pdf_content=pdf_content,
            matter_id=str(matter_id),
            key_date=str(row['Key Date']),
            purchase_amount=float(row['Purchase Amount']),
            kind=row['Kind'],
            purchaser_names=row['Purchaser Names'],
            seller_names=row['Seller Names'],
        ).with_inputs('pdf_content', 'matter_id')

        examples.append(example)

    split_idx = int(len(examples) * 0.8)
    return examples[:split_idx], examples[split_idx:]

# =============================================================================
# Metric Function
# =============================================================================

def metric(gold, pred, trace=None, pred_name=None, pred_trace=None):
    """Multi-field extraction metric with partial credit."""
    scores = {}
    feedback_parts = []

    # Date (exact)
    if str(gold.key_date) == str(getattr(pred, 'key_date', '')):
        scores['key_date'] = 1.0
    else:
        scores['key_date'] = 0.0
        feedback_parts.append(f"key_date: expected '{gold.key_date}'")

    # Amount (1% tolerance)
    try:
        gold_amt = float(gold.purchase_amount)
        pred_amt = float(getattr(pred, 'purchase_amount', 0))
        if abs(gold_amt - pred_amt) / gold_amt < 0.01:
            scores['purchase_amount'] = 1.0
        else:
            scores['purchase_amount'] = 0.0
            feedback_parts.append(f"purchase_amount: expected {gold_amt}")
    except:
        scores['purchase_amount'] = 0.0
        feedback_parts.append("purchase_amount: parse error")

    # Kind (case-insensitive)
    if str(gold.kind).upper() == str(getattr(pred, 'kind', '')).upper():
        scores['kind'] = 1.0
    else:
        scores['kind'] = 0.0
        feedback_parts.append(f"kind: expected '{gold.kind}'")

    # Names (fuzzy)
    scores['purchaser_names'] = fuzzy_name_match(
        gold.purchaser_names, getattr(pred, 'purchaser_names', '')
    )
    scores['seller_names'] = fuzzy_name_match(
        gold.seller_names, getattr(pred, 'seller_names', '')
    )

    # Weighted average
    weights = {
        'key_date': 0.2, 'purchase_amount': 0.2, 'kind': 0.1,
        'purchaser_names': 0.25, 'seller_names': 0.25
    }
    total = sum(scores[k] * weights[k] for k in weights)

    return dspy.Prediction(
        score=total,
        feedback="; ".join(feedback_parts) if feedback_parts else "All correct!"
    )

# =============================================================================
# Main
# =============================================================================

def main():
    lm = dspy.LM(LM_MODEL)
    reflection_lm = dspy.LM(REFLECTION_LM)
    dspy.configure(lm=lm)

    train_data, val_data = load_data()
    print(f"Loaded {len(train_data)} training, {len(val_data)} validation examples")

    optimizer = GEPA(
        metric=metric,
        auto="medium",
        reflection_lm=reflection_lm,
        server_url=SERVER_URL,
        project_name=PROJECT_NAME,
        capture_lm_calls=True,
        verbose=True,
    )

    print("Starting GEPA optimization...")
    optimized = optimizer.compile(
        student=program,
        trainset=train_data,
        valset=val_data,
    )

    optimized.save("extraction_optimized.json")
    print("Saved optimized program to extraction_optimized.json")

    if val_data:
        print("\nTesting optimized program...")
        test = val_data[0]
        result = optimized(pdf_content=test.pdf_content, matter_id=test.matter_id)
        print(f"Extracted key_date: {result.key_date}")
        print(f"Expected key_date: {test.key_date}")

    return optimized


if __name__ == "__main__":
    main()
```

---

## requirements.txt

```
dspy-ai>=2.5.0
dspy-gepa-logger>=0.2.0a0
pandas>=2.0.0
python-dotenv>=1.0.0
```

Add additional packages based on task:
- For fuzzy matching: `rapidfuzz>=3.0.0` (optional, difflib is built-in)
- For specific file types: relevant parsing libraries
